from datapackage_pipelines_plus_plus.base_processors.base import BaseProcessor
import logging, tabulator, xlrd, os, collections
from ruamel.yaml import YAML


class Processor(BaseProcessor):

    def _filter_datapackage(self, datapackage):
        pipeline_steps = []
        max_peek_rows = int(self._parameters.get("max-peek-rows", 15))
        for source_number, source in enumerate(datapackage["sources"]):
            source_name = source["name"]
            source_path = source["path"]
            source_ext = os.path.splitext(source_path)[1].lower()
            if source_ext == ".xls" or source_ext == ".xlsx":
                xlrd_book = xlrd.open_workbook(source_path)
                for sheet_num, sheet_name in enumerate(xlrd_book.sheet_names(), start=1):
                    peek_rows = []
                    with tabulator.Stream(source_path, sheet=sheet_num) as stream:
                        for row_num, row in enumerate(stream.iter()):
                            if row_num < max_peek_rows:
                                peek_rows.append(row)
                            else:
                                break
                    headers = []
                    max_header_rows = int(self._parameters.get("max-header-rows", 5))
                    for row_num, row in enumerate(peek_rows):
                        for col_num, value in enumerate(row):
                            value = str(value)
                            if len(headers) == col_num:
                                headers.append([])
                            if len(headers[col_num]) < max_header_rows:
                                headers[col_num].append(value)
                    headers = ["rows: "+" -- ".join(header) for header in headers]
                    parameters = dict(collections.OrderedDict([("name", "{}-sheet-{}".format(source_name, sheet_num)),
                                                               ("source-sheet-name", sheet_name),
                                                               ("url", source_path),
                                                               ("format", "xls"),
                                                               ("skip_rows", 1),
                                                               ("sheet", sheet_num),
                                                               ("headers", headers)]))
                    pipeline_steps.append(dict(collections.OrderedDict([("run", "add_resource"),
                                                                   ("parameters", parameters)])))
            else:
                raise Exception("extension not supported: {}".format(source_ext))
        source_spec = dict(collections.OrderedDict([("__comment__", [
            "This file was generated by the autoload processor.",
            "The pipelines generated here should be copied to the plus_plus.source-spec.yaml file and modified there.",
            "This file should be kept as-is for reference and as  documentation of how the data was loaded."
        ]), ("load",  {"pipeline": pipeline_steps})]))
        yaml = YAML()
        yaml.allow_unicode = True
        with open ("autoload_source_spec.yaml", "w") as f:
            yaml.dump(source_spec, f)
        logging.info("saved autoload_source_spec.yaml")
        return datapackage


if __name__ == "__main__":
    Processor.main()
